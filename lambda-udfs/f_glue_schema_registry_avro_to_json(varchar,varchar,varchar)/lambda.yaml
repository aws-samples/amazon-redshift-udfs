AWSTemplateFormatVersion: '2010-09-09'
Parameters:
  S3Bucket:
    Description: the S3 Bucket containing the Lambda layer
    Type: String
  S3Key:
    Description: the S3 Key containing the Lambda layer
    Type: String
Conditions:
  NoS3Prefix: !Equals [!Ref S3Key, '']
Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
          Version: 2012-10-17
          Statement:
            -
              Effect: Allow
              Principal:
                Service:
                  - lambda.amazonaws.com
              Action:
                - sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - !Sub "arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole"
      Policies:
          -
            PolicyName: CloudwatchLogs
            PolicyDocument:
              Version: 2012-10-17
              Statement:
                -
                  Effect: Allow
                  Action:
                    - logs:CreateLogGroup
                  Resource:
                    - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
                -
                  Effect: Allow
                  Action:
                    - logs:CreateLogStream
                    - logs:PutLogEvents
                  Resource:
                    - !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*"
                -
                  Effect: Allow
                  Action:
                    - glue:GetSchemaVersion
                  Resource:
                    - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:registry/*"
                    - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:schema/*/*"
  AvroLayer:
    Type: AWS::Lambda::LayerVersion
    Properties: 
      CompatibleRuntimes: 
        - python3.9
      Content: 
        S3Bucket: !Ref S3Bucket
        S3Key: !If ["NoS3Prefix", "avro_1.11.2.zip", !Sub "${S3Key}/avro_1.11.2.zip"]
      Description: Python 3.9 Avro library
      LayerName: avro
  LambdaUDFFunction:
     Type: "AWS::Lambda::Function"
     Properties:
       FunctionName: f-glue-schema-registry-avro-to-json-varchar-varchar-varchar
       Role: !GetAtt 'LambdaRole.Arn'
       Handler: index.handler
       Runtime: python3.9
       Timeout: 300
       Layers:
        - !Ref AvroLayer
       Code:
         ZipFile: |
          import functools
          import io
          import json
          import os

          import avro.io
          import avro.schema
          import boto3

          glue = boto3.client("glue")


          @functools.lru_cache(maxsize=32)
          def _get_schema(registry_name: str, stream_name: str) -> avro.schema.Schema:
              """Get an Avro schema from the registry by stream name.

              :param stream_name: The stream name for the schema to request
              :returns: The Avro schema object
              """
              schema_resp = glue.get_schema_version(
                  SchemaId={"RegistryName": registry_name, "SchemaName": stream_name},
                  SchemaVersionNumber={"LatestVersion": True},
              )
              schema = schema_resp["SchemaDefinition"]
              return avro.schema.parse(schema)


          def _avro_to_json(registry_name: str, stream_name: str, data: str) -> str:
              """Decode a single Hex-encoded Avro datum using the schema associated with the stream name.

              :param registry_name: The Glue Schema Registry name for the schema
              :param stream_name: The stream name for the data
              :param data: The Hex-encoded Avro binary data
              :returns: A JSON encoded version of the data
              """
              schema = _get_schema(registry_name, stream_name)
              data_bytes = io.BytesIO(bytes.fromhex(data))
              decoder = avro.io.BinaryDecoder(data_bytes)
              reader = avro.io.DatumReader(schema)
              decoded = reader.read(decoder)
              return json.dumps(decoded)


          def handler(event, context):
              try:
                  results = []
                  for registry_name, stream_name, data in event["arguments"]:
                      results.append(_avro_to_json(registry_name, stream_name, data))
                  return json.dumps(
                      {
                          "success": True,
                          "num_records": event["num_records"],
                          "results": results,
                      }
                  )
              except Exception as e:
                  return json.dumps(
                      {
                          "success": False,
                          "error_msg": f"Error processing Lambda event. Error: {e}. Event: {event}",
                      }
                  )
